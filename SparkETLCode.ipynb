{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. PySpark Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Setting the environment for the PySpark\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/opt/cloudera/parcels/Anaconda/bin/python\"\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/java/jdk1.8.0_232-cloudera/jre\"\n",
    "os.environ[\"SPARK_HOME\"]=\"/opt/cloudera/parcels/SPARK2-2.3.0.cloudera2-1.cdh5.13.3.p0.316101/lib/spark2/\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.6-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-206.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkETLCode</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f645c5c2190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a Spark Session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SparkETLCode').master(\"local\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-0-206.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0.cloudera2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkETLCode</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=SparkETLCode>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accessing SparkContext from SparkSession\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Importing the Bank Data from HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is loaded using the files in the provided location of HDFS and with a well-defined schema.\n",
    "The total count, schema and data is verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a schema that matches with the data present in the HDFS.\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, BinaryType, FloatType\n",
    "fileSchema = StructType([StructField('year', IntegerType(),True),\n",
    "                        StructField('month', StringType(),True),\n",
    "                        StructField('day', IntegerType(),True),\n",
    "                        StructField('weekday', StringType(),True),\n",
    "                        StructField('hour', IntegerType(),True),\n",
    "                        StructField('atm_status', StringType(),True),\n",
    "                        StructField('atm_id', StringType(),True),\n",
    "                        StructField('atm_manufacturer', StringType(),True),\n",
    "                        StructField('atm_location', StringType(),True),\n",
    "                        StructField('atm_streetname', StringType(),True),\n",
    "                        StructField('atm_street_number', IntegerType(),True),\n",
    "                        StructField('atm_zipcode', IntegerType(),True),\n",
    "                        StructField('atm_lat', DoubleType(),True),\n",
    "                        StructField('atm_lon', DoubleType(),True),\n",
    "                        StructField('currency', StringType(),True),\n",
    "                        StructField('card_type', StringType(),True),\n",
    "                        StructField('transaction_amount', IntegerType(),True),\n",
    "                        StructField('service', StringType(),True), \n",
    "                        StructField('message_code', StringType(),True),\n",
    "                        StructField('message_text', StringType(),True),\n",
    "                        StructField('weather_lat', DoubleType(),True),\n",
    "                        StructField('weather_lon', DoubleType(),True),\n",
    "                        StructField('weather_city_id', IntegerType(),True),\n",
    "                        StructField('weather_city_name', StringType(),True),\n",
    "                        StructField('temp', DoubleType(),True), \n",
    "                        StructField('pressure', IntegerType(),True),\n",
    "                        StructField('humidity', IntegerType(),True),\n",
    "                        StructField('wind_speed', IntegerType(),True),\n",
    "                        StructField('wind_deg', IntegerType(),True),\n",
    "                        StructField('rain_3h', DoubleType(),True),\n",
    "                        StructField('clouds_all', IntegerType(),True),\n",
    "                        StructField('weather_id', IntegerType(),True), \n",
    "                        StructField('weather_main', StringType(),True),\n",
    "                        StructField('weather_description', StringType(),True),\n",
    "                        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data in the dataframe using schema as defined above.\n",
    "bank_data = spark.read.load(\"/user/root/ETL_Project/bank_data_import/\", format=\"csv\", schema = fileSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- atm_status: string (nullable = true)\n",
      " |-- atm_id: string (nullable = true)\n",
      " |-- atm_manufacturer: string (nullable = true)\n",
      " |-- atm_location: string (nullable = true)\n",
      " |-- atm_streetname: string (nullable = true)\n",
      " |-- atm_street_number: integer (nullable = true)\n",
      " |-- atm_zipcode: integer (nullable = true)\n",
      " |-- atm_lat: double (nullable = true)\n",
      " |-- atm_lon: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- card_type: string (nullable = true)\n",
      " |-- transaction_amount: integer (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- message_code: string (nullable = true)\n",
      " |-- message_text: string (nullable = true)\n",
      " |-- weather_lat: double (nullable = true)\n",
      " |-- weather_lon: double (nullable = true)\n",
      " |-- weather_city_id: integer (nullable = true)\n",
      " |-- weather_city_name: string (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- pressure: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- wind_speed: integer (nullable = true)\n",
      " |-- wind_deg: integer (nullable = true)\n",
      " |-- rain_3h: double (nullable = true)\n",
      " |-- clouds_all: integer (nullable = true)\n",
      " |-- weather_id: integer (nullable = true)\n",
      " |-- weather_main: string (nullable = true)\n",
      " |-- weather_description: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema for the imported data\n",
    "bank_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the total number of rows loaded in spark\n",
    "bank_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-----------------------+\n",
      "|year|month  |day|weekday|hour|atm_status|atm_id|atm_manufacturer|atm_location|atm_streetname     |atm_street_number|atm_zipcode|atm_lat|atm_lon|currency|card_type |transaction_amount|service   |message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|temp  |pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main|weather_description    |\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-----------------------+\n",
      "|2017|January|1  |Sunday |0   |Active    |1     |NCR             |NÃƒÂ¦stved  |Farimagsvej        |8                |4700       |55.233 |11.763 |DKK     |MasterCard|5643              |Withdrawal|null        |null        |55.23      |11.761     |2616038        |Naestved         |281.15|1014    |87      |7         |260     |0.215  |92        |500       |Rain        |light rain             |\n",
      "|2017|January|1  |Sunday |0   |Inactive  |2     |NCR             |Vejgaard    |Hadsundvej         |20               |9000       |57.043 |9.95   |DKK     |MasterCard|1764              |Withdrawal|null        |null        |57.048     |9.935      |2616235        |NÃƒÂ¸rresundby   |280.64|1020    |93      |9         |250     |0.59   |92        |500       |Rain        |light rain             |\n",
      "|2017|January|1  |Sunday |0   |Inactive  |2     |NCR             |Vejgaard    |Hadsundvej         |20               |9000       |57.043 |9.95   |DKK     |VISA      |1891              |Withdrawal|null        |null        |57.048     |9.935      |2616235        |NÃƒÂ¸rresundby   |280.64|1020    |93      |9         |250     |0.59   |92        |500       |Rain        |light rain             |\n",
      "|2017|January|1  |Sunday |0   |Inactive  |3     |NCR             |Ikast       |RÃƒÂ¥dhusstrÃƒÂ¦det|12               |7430       |56.139 |9.154  |DKK     |VISA      |4166              |Withdrawal|null        |null        |56.139     |9.158      |2619426        |Ikast            |281.15|1011    |100     |6         |240     |0.0    |75        |300       |Drizzle     |light intensity drizzle|\n",
      "|2017|January|1  |Sunday |0   |Active    |4     |NCR             |Svogerslev  |BrÃƒÂ¸nsager       |1                |4000       |55.634 |12.018 |DKK     |MasterCard|5153              |Withdrawal|null        |null        |55.642     |12.08      |2614481        |Roskilde         |280.61|1014    |87      |7         |260     |0.0    |88        |701       |Mist        |mist                   |\n",
      "+----+-------+---+-------+----+----------+------+----------------+------------+-------------------+-----------------+-----------+-------+-------+--------+----------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+------+--------+--------+----------+--------+-------+----------+----------+------------+-----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the first row to verify if there are any errors in the schema or the parameters passed while importing.\n",
    "bank_data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Creation of Dimension Tables using Loaded Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Creating DIM_LOCATION table using the bank_data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The required columns are selected from the loaded data into a dataframe. Alias are provided wherever required.\n",
    "- Duplicates are removed from the dataframe.\n",
    "- A unique row id is assigned to every row in the dataframe.\n",
    "- Columns are rearranged and count is verified after operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pyspark functions and windows\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+--------------+-------------+-------+------+-----+\n",
      "|location              |streetname    |street_number|zipcode|lat   |lon  |\n",
      "+----------------------+--------------+-------------+-------+------+-----+\n",
      "|NykÃƒÂ¸bing Mors Lobby|Kirketorvet   |1            |7900   |56.795|8.86 |\n",
      "|Nibe                  |Torvet        |1            |9240   |56.983|9.639|\n",
      "|Skipperen             |Vestre Alle   |2            |9000   |57.034|9.908|\n",
      "|Viborg                |Toldboden     |3            |8800   |56.448|9.401|\n",
      "|Vadum                 |Ellehammersvej|43           |9430   |57.118|9.861|\n",
      "+----------------------+--------------+-------------+-------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting required columns from the dataframe to create the required table.\n",
    "dim_location = bank_data.select(col(\"atm_location\").alias(\"location\"),col(\"atm_streetname\").alias(\"streetname\"),\n",
    "                                col(\"atm_street_number\").alias(\"street_number\"), col(\"atm_zipcode\").alias(\"zipcode\"),\n",
    "                                col(\"atm_lat\").alias(\"lat\"), col(\"atm_lon\").alias(\"lon\"))\n",
    "\n",
    "# Dropping duplicates from the dim_location dataframe\n",
    "dim_location_distincts = dim_location.dropDuplicates()\n",
    "dim_location_distincts.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------+--------------+-------------+-------+------+-----+\n",
      "|location_id|location              |streetname    |street_number|zipcode|lat   |lon  |\n",
      "+-----------+----------------------+--------------+-------------+-------+------+-----+\n",
      "|location_1 |NykÃƒÂ¸bing Mors Lobby|Kirketorvet   |1            |7900   |56.795|8.86 |\n",
      "|location_2 |Nibe                  |Torvet        |1            |9240   |56.983|9.639|\n",
      "|location_3 |Skipperen             |Vestre Alle   |2            |9000   |57.034|9.908|\n",
      "|location_4 |Viborg                |Toldboden     |3            |8800   |56.448|9.401|\n",
      "|location_5 |Vadum                 |Ellehammersvej|43           |9430   |57.118|9.861|\n",
      "+-----------+----------------------+--------------+-------------+-------+------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assigning a unique value (primary key) to the location_id column \n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "dim_location = dim_location_distincts.withColumn(\"location_id\", row_number().over(window))\n",
    "def location_Id(value):\n",
    "    return \"location_\" + str(value)\n",
    "location_id_udf = udf(location_Id, StringType())\n",
    "dim_location = dim_location.withColumn(\"location_id\", location_id_udf(\"location_id\"))\n",
    "\n",
    "# Rearranging the columns\n",
    "dim_location = dim_location.select('location_id','location','streetname','street_number','zipcode','lat','lon')\n",
    "dim_location.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the count of the rows in the dim_location table\n",
    "dim_location.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Creating DIM_ATM table using the bank_data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The required columns are selected from the loaded data into a dataframe. Alias are provided wherever required.\n",
    "- Duplicates are removed from the dataframe.\n",
    "- The dataframe is joined with the location dataframe using lat and lon columns.\n",
    "- A unique row id is assigned to every row in the dataframe.\n",
    "- Columns are rearranged and count is verified after operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------+------+\n",
      "|atm_number|atm_manufacturer|lat   |lon   |\n",
      "+----------+----------------+------+------+\n",
      "|113       |Diebold Nixdorf |55.398|11.342|\n",
      "|54        |NCR             |56.745|8.949 |\n",
      "|104       |NCR             |57.049|9.922 |\n",
      "|18        |Diebold Nixdorf |56.448|9.401 |\n",
      "|8         |NCR             |56.762|8.867 |\n",
      "+----------+----------------+------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting required columns from the dataframe to create the required table.\n",
    "dim_atm = bank_data.select(col(\"atm_id\").alias(\"atm_number\"), col(\"atm_manufacturer\"),\n",
    "                           col('atm_lat').alias('lat'),col('atm_lon').alias('lon'))\n",
    "\n",
    "# Dropping duplicates from the dim_atm dataframe\n",
    "dim_atm_distincts = dim_atm.dropDuplicates()\n",
    "dim_atm_distincts.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+----------------+---------------+\n",
      "|atm_id|atm_number|atm_manufacturer|atm_location_id|\n",
      "+------+----------+----------------+---------------+\n",
      "|atm_1 |113       |Diebold Nixdorf |location_104   |\n",
      "|atm_2 |113       |Diebold Nixdorf |location_90    |\n",
      "|atm_3 |54        |NCR             |location_93    |\n",
      "|atm_4 |104       |NCR             |location_63    |\n",
      "|atm_5 |104       |NCR             |location_25    |\n",
      "+------+----------+----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining the table with Location Table to get the required location_id.\n",
    "condition = [dim_atm_distincts.lat == dim_location.lat, dim_atm_distincts.lon == dim_location.lon]\n",
    "dim_atm_joined = dim_atm_distincts.join(dim_location, condition, 'left_outer')\n",
    "\n",
    "# Assigning a unique value (primary key) to the atm_id column \n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "dim_atm_joined = dim_atm_joined.withColumn(\"atm_id\", row_number().over(window))\n",
    "def atm_Id(value):\n",
    "    return \"atm_\" + str(value)\n",
    "atm_id_udf = udf(atm_Id, StringType())\n",
    "dim_atm_joined = dim_atm_joined.withColumn(\"atm_id\", atm_id_udf(\"atm_id\"))\n",
    "\n",
    "# Rearranging the columns\n",
    "dim_atm_joined = dim_atm_joined.select('atm_id','atm_number','atm_manufacturer',col('location_id').alias('atm_location_id'))\n",
    "dim_atm_joined.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifing the total count for the dim_atm dimension.\n",
    "dim_atm_joined.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creating DIM_CARD_TYPE table using the bank_data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The required columns are selected from the loaded data into a dataframe. Alias are provided wherever required.\n",
    "- Duplicates are removed from the dataframe.\n",
    "- A unique row id is assigned to every row in the dataframe.\n",
    "- Columns are rearranged and count is verified after operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|card_type         |\n",
      "+------------------+\n",
      "|Dankort - on-us   |\n",
      "|CIRRUS            |\n",
      "|HÃƒÂ¦vekort       |\n",
      "|VISA              |\n",
      "|Mastercard - on-us|\n",
      "+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting required columns from the dataframe to create the required table.\n",
    "dim_card_type = bank_data.select(col(\"card_type\"))\n",
    "\n",
    "# Dropping duplicates from the dim_card_type dataframe\n",
    "dim_card_type = dim_card_type.dropDuplicates()\n",
    "dim_card_type.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------+\n",
      "|card_type         |card_type_id|\n",
      "+------------------+------------+\n",
      "|Dankort - on-us   |card_type_1 |\n",
      "|CIRRUS            |card_type_2 |\n",
      "|HÃƒÂ¦vekort       |card_type_3 |\n",
      "|VISA              |card_type_4 |\n",
      "|Mastercard - on-us|card_type_5 |\n",
      "+------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assigning a unique value (primary key) to the atm_id column \n",
    "\n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "dim_card_type = dim_card_type.withColumn(\"card_type_id\", row_number().over(window))\n",
    "def card_type_Id(value):\n",
    "    return \"card_type_\" + str(value)\n",
    "card_type_udf = udf(card_type_Id, StringType())\n",
    "dim_card_type = dim_card_type.withColumn(\"card_type_id\", card_type_udf(\"card_type_id\"))\n",
    "\n",
    "#Rearranging the columns\n",
    "dim_card_type_old = dim_card_type.select('card_type_id','card_type')\n",
    "dim_card_type.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifing the total count for the dim_atm dimension.\n",
    "dim_card_type.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Creating DIM_DATE table using the bank_data dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The required columns are selected from the loaded data into a dataframe. Alias are provided wherever required.\n",
    "- Duplicates are removed from the dataframe.\n",
    "- Date is created with year, month and day first. Then the date is converted in timestamp using expr and to_timestamp function.\n",
    "- A unique row id is assigned to every row in the dataframe.\n",
    "- Columns are rearranged and count is verified after operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+---+----+--------+\n",
      "|year|month  |day|hour|weekday |\n",
      "+----+-------+---+----+--------+\n",
      "|2017|January|1  |9   |Sunday  |\n",
      "|2017|January|3  |5   |Tuesday |\n",
      "|2017|January|8  |19  |Sunday  |\n",
      "|2017|January|21 |3   |Saturday|\n",
      "|2017|January|23 |21  |Monday  |\n",
      "+----+-------+---+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecting required columns from the dataframe to create the required table.\n",
    "dim_date = bank_data.select(col(\"year\"), \"month\", \"day\", \"hour\", \"weekday\")\n",
    "\n",
    "# Dropping duplicates from the dim_date dataframe\n",
    "dim_date = dim_date.dropDuplicates()\n",
    "dim_date.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+-------+---+----+--------+\n",
      "|date_id|full_date_time     |year|month  |day|hour|weekday |\n",
      "+-------+-------------------+----+-------+---+----+--------+\n",
      "|date_1 |2017-01-01 09:00:00|2017|January|1  |9   |Sunday  |\n",
      "|date_2 |2017-01-03 05:00:00|2017|January|3  |5   |Tuesday |\n",
      "|date_3 |2017-01-08 19:00:00|2017|January|8  |19  |Sunday  |\n",
      "|date_4 |2017-01-21 03:00:00|2017|January|21 |3   |Saturday|\n",
      "|date_5 |2017-01-23 21:00:00|2017|January|23 |21  |Monday  |\n",
      "+-------+-------------------+----+-------+---+----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the date from year, month, and day column\n",
    "dim_date = dim_date.withColumn('month_int', from_unixtime(\n",
    "    unix_timestamp(col(\"month\"),'MMM'),'MM')).withColumn(\n",
    "    'date',date_format(concat_ws(\"-\",col('year'),col('month_int'),col('day')),\"yyyy-MM-dd\").cast('date'))\n",
    "\n",
    "# Creating the required timestamp column after merging the date\n",
    "dim_date = dim_date.withColumn('date',expr(\"date || '-' || hour || ':00:00'\")\n",
    "                   ).withColumn('date',to_timestamp('date','yyyy-MM-dd-HH:mm:ss'))\n",
    "\n",
    "# Assigning a unique value (primary key) to the atm_id column \n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "dim_date = dim_date.withColumn(\"date_id\", row_number().over(window))\n",
    "def date_Id(value):\n",
    "    return \"date_\" + str(value)\n",
    "date_udf = udf(date_Id, StringType())\n",
    "dim_date = dim_date.withColumn(\"date_id\", date_udf(\"date_id\"))\n",
    "\n",
    "# Rearranging the columns\n",
    "dim_date = dim_date.select(col('date_id'),col('date').alias('full_date_time'),col(\"year\"), \"month\", \"day\", \"hour\", \"weekday\")\n",
    "dim_date.show(5,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date_id: string (nullable = true)\n",
      " |-- full_date_time: timestamp (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: string (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verifying the schema\n",
    "dim_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8685"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifing the total count for the dim_atm dimension.\n",
    "dim_date.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Creation of Fact Table using Loaded Data and Dimension Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A new dataframe is created for the fact table using the previous loaded data.\n",
    "- The dataframe is joined with DIM_LOCATION table using all the columns in the dataframe.\n",
    "- The resultant is joined with DIM_CARD_TYPE and DIM_DATE and DIM_ATM table respectively using the candidate keys for the table.\n",
    "- The extra columns that might get created after join are dropped off.\n",
    "- Unique ID is assigned to the fact table and the columns are rearranged according to the target schema.\n",
    "- The result is verified printing the data and matching the count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copying the loaded into another dataframe to create the fact table\n",
    "fact_atm_trans = bank_data.select('*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+------------+------------+-------+------+\n",
      "|atm_status|currency|transaction_amount|service   |message_code|message_text|weather_lat|weather_lon|weather_city_id|weather_city_name|temp   |pressure|humidity|wind_speed|wind_deg|rain_3h|clouds_all|weather_id|weather_main|weather_description|location_id |card_type_id|date_id|atm_id|\n",
      "+----------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+------------+------------+-------+------+\n",
      "|Inactive  |DKK     |8933              |Withdrawal|null        |null        |57.464     |9.982      |2620214        |Hjorring         |283.921|1029    |79      |8         |241     |0.0    |0         |800       |Clear       |Sky is Clear       |location_101|card_type_8 |date_61|atm_40|\n",
      "|Inactive  |DKK     |274               |Withdrawal|null        |null        |57.464     |9.982      |2620214        |Hjorring         |283.921|1029    |79      |8         |241     |0.0    |0         |800       |Clear       |Sky is Clear       |location_101|card_type_8 |date_61|atm_40|\n",
      "+----------+--------+------------------+----------+------------+------------+-----------+-----------+---------------+-----------------+-------+--------+--------+----------+--------+-------+----------+----------+------------+-------------------+------------+------------+-------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Joining the table with DIM_LOCATION table to get the required location_id.\n",
    "condition = [fact_atm_trans.atm_location == dim_location.location, fact_atm_trans.atm_lat == dim_location.lat,\n",
    "             fact_atm_trans.atm_lon == dim_location.lon, fact_atm_trans.atm_streetname == dim_location.streetname, \n",
    "            fact_atm_trans.atm_street_number == dim_location.street_number, fact_atm_trans.atm_zipcode == dim_location.zipcode]\n",
    "fact_atm_trans = fact_atm_trans.join(dim_location, condition, 'left_outer').select('*')\n",
    "\n",
    "# Joining the table with DIM_CARD_TYPE table to get the required card_type_id.\n",
    "condition = [fact_atm_trans.card_type == dim_card_type.card_type]\n",
    "fact_atm_trans = fact_atm_trans.join(dim_card_type, condition, 'left_outer').select('*')\n",
    "\n",
    "# Joining the table with DIM_DATE table to get the required date_id.\n",
    "condition = [fact_atm_trans.year == dim_date.year, fact_atm_trans.month == dim_date.month,\n",
    "             fact_atm_trans.day == dim_date.day, fact_atm_trans.hour == dim_date.hour,\n",
    "            fact_atm_trans.weekday == dim_date.weekday]\n",
    "fact_atm_trans = fact_atm_trans.join(dim_date, condition, 'left_outer').select('*')\n",
    "\n",
    "# Joining the table with DIM_ATM table to get the required atm_id.\n",
    "fact_atm_trans = fact_atm_trans.withColumnRenamed(\"atm_id\",\"atm_id_fact\")\n",
    "condition = [fact_atm_trans.atm_id_fact == dim_atm_joined.atm_number, fact_atm_trans.atm_manufacturer == dim_atm_joined.atm_manufacturer,\n",
    "             fact_atm_trans.location_id == dim_atm_joined.atm_location_id]\n",
    "fact_atm_trans = fact_atm_trans.join(dim_atm_joined, condition, 'left_outer').select('*')\n",
    "\n",
    "\n",
    "columns_to_drop = ['atm_location','atm_streetname','atm_street_number','atm_zipcode',\n",
    "                   'atm_lat','atm_lon','location','streetname','street_number','zipcode','lat','lon','card_type',\n",
    "                  'year','month','day','hour','weekday','full_date_time','atm_manufacturer','atm_number','atm_id_fact','atm_location_id']\n",
    "fact_atm_trans = fact_atm_trans.drop(*columns_to_drop)\n",
    "fact_atm_trans.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-------------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+-------------------+\n",
      "|trans_id|atm_id|weather_location_id|date_id|card_type_id|atm_status|currency|service   |transaction_amount|message_code|message_text|rain_3h|clouds_all|weather_id|weather_main|weather_description|\n",
      "+--------+------+-------------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+-------------------+\n",
      "|trans_1 |atm_40|location_101       |date_61|card_type_8 |Inactive  |DKK     |Withdrawal|8933              |null        |null        |0.0    |0         |800       |Clear       |Sky is Clear       |\n",
      "|trans_2 |atm_40|location_101       |date_61|card_type_8 |Inactive  |DKK     |Withdrawal|274               |null        |null        |0.0    |0         |800       |Clear       |Sky is Clear       |\n",
      "+--------+------+-------------------+-------+------------+----------+--------+----------+------------------+------------+------------+-------+----------+----------+------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assigning a unique value (primary key) to the atm_id column \n",
    "window = Window.orderBy(monotonically_increasing_id())\n",
    "fact_atm_trans = fact_atm_trans.withColumn('trans_id', row_number().over(window))\n",
    "def fact_atm_trans_Id(value):\n",
    "    return \"trans_\" + str(value)\n",
    "fact_atm_trans_udf = udf(fact_atm_trans_Id, StringType())\n",
    "fact_atm_trans = fact_atm_trans.withColumn('trans_id', fact_atm_trans_udf('trans_id'))\n",
    "\n",
    "#Rearranging the columns\n",
    "fact_atm_trans = fact_atm_trans.select('trans_id','atm_id',col('location_id').alias('weather_location_id'),'date_id',\n",
    "                                       'card_type_id','atm_status','currency','service','transaction_amount','message_code',\n",
    "                                      'message_text','rain_3h','clouds_all','weather_id','weather_main','weather_description')\n",
    "fact_atm_trans.show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2468572"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the count of the fact table:\n",
    "fact_atm_trans.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Exporting the required dataframes into Amazon S3 buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data to the s3 bucket using write method.\n",
    "dim_location.write.csv(\"s3a://shardul-etl-bank-data/Dim_Location/Dim_Location.csv\", mode=\"overwrite\")\n",
    "dim_atm_joined.write.csv(\"s3a://shardul-etl-bank-data/Dim_ATM/Dim_ATM.csv\", mode=\"overwrite\")\n",
    "dim_card_type.write.csv(\"s3a://shardul-etl-bank-data/Dim_Card_Type/Dim_Card_Type.csv\", mode=\"overwrite\")\n",
    "dim_date.write.parquet(\"s3a://shardul-etl-bank-data/Dim_Date/Dim_Date.parquet\", mode=\"overwrite\")\n",
    "fact_atm_trans.write.csv(\"s3a://shardul-etl-bank-data/Fact_Transaction/Fact_Transaction.csv\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
